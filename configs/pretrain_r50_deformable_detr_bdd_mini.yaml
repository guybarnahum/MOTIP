# configs/pretrain_r50_deformable_detr_bdd_mini.yaml

# Base Architecture (Inherit from DanceTrack)
SUPER_CONFIG_PATH: ./configs/r50_deformable_detr_motip_dancetrack.yaml
root_path: "."

# Dataset & Classes
# DanceTrack base defaults to 1 class. We MUST override this to 2.
NUM_CLASSES: 2

# Weights Initialization
# We load the weights, but NOT the optimizer state (shapes mismatch)
DETR_PRETRAIN: ./pretrains/motip_dancetrack.pth
RESUME_OPTIMIZER: False
RESUME_SCHEDULER: False

# Reduce Memory: Force short video clips (2 frames instead of default 5)
sampler_lengths: [2]

# Training Strategy (Mini Dataset)
EPOCHS: 10
LR: 2.0e-5 
LR_DROP: 7
LR_BACKBONE: 2.0e-6
LR_WARMUP_EPOCHS: 0

# Hardware Safety (A10G)
BATCH_SIZE: 1
ACCUMULATE_STEPS: 1
NUM_WORKERS: 4

# Global flag to enable aggressive memory saving techniques
# Set to True for 24GB cards. Set to False for 48GB+ cards.
MEMORY_EFFICIENT: True
USE_DECODER_CHECKPOINT: True

# 6. Output
# We explicitly set these to null so the python script uses 
# the EXP_NAME (timestamped) generated by the train bash script.
OUTPUT_DIR: null
OUTPUTS_DIR: null

# Cap the maximum input size to 1000 (down from default 1333)
# This significantly reduces memory usage for Attention layers.
train_transform:
  max_size: 1000
  min_sizes: [600, 700, 800]  # Slightly reduced min sizes
